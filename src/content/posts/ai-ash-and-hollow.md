---
title: "How I Engineered AI Into My Creative Workflow for Ash and Hollow"
filename: "AI-Ash-And-Hollow.md"
author: "Jonjoe Whitfield"
tags: ["Ash and Hollow", "AI", "worldbuilding", "Obsidian", "workflow", "JSON", "creative process"]
---

I didn’t set out to build a 150,000-word universe — but here we are.

I’ve been a bit quieter here over the past year and a half — caught up in personal projects, personal growth, and the occasional deep rabbit hole. One of those rabbit holes became Ash and Hollow, a sprawling universe that’s part creative challenge, part technical experiment, and part “maybe I’ve overcomplicated this.” Along the way, I’ve learned how AI can support complex, evolving work while keeping full creative control. Here’s the journey — the problems I faced, the systems I built, and how it’s reshaped my approach to storytelling, worldbuilding, and technology.

## The Project — What It Is and How Vast It Is
Ash and Hollow is built over thousands of hours of dedicated work. It blends deep lore, character-driven storytelling, and a layered political and cultural landscape across multiple factions, timelines, and locations.

The project contains:

150,000+ words of developed lore.
300+ markdown files housing worldbuilding, manuscript chapters, and reference material.
200+ glossary terms defining key concepts, factions, items, and cultural elements.
Thousands of wiki-style relationships linking characters, events, places, and ideas.

Its scope covers fully mapped character arcs, a detailed glossary, act-by-act manuscript planning, scene drafts, and interwoven historical events, myths, and cultural practices that give the world depth. With so much content, every detail must remain consistent.

## My Challenges — Keeping Everything Consistent and Aligned
The interconnected nature of Ash and Hollow means that even small changes — renaming a faction, altering a historical event, adjusting a character detail — can ripple across hundreds of files. Tracking these changes manually was time-consuming and error-prone. Cross-referencing lore, spotting contradictions, and keeping every element in alignment became a major bottleneck.

## Integrating AI — The Problems This Solved and the New Problems It Created
I turned to AI imagining it could manage everything from continuity checks to structural edits. I started with ChatGPT, feeding it large chunks of content. Initially, the results were excellent — it surfaced contradictions, offered fresh perspectives, and acted as a second set of eyes on lore alignment.

But issues appeared quickly:

Working memory limits — I couldn’t just dump everything into one session and expect stability.
Session instability — Long-running chats became less reliable over time.
Loss of important details — Large text dumps caused omissions and forgotten context.
Hallucinations — Confident but inaccurate statements.

Believing the issue was the product, not the process, I tried other models. Gemini’s 2 million token capacity seemed like a fix, but it only delayed the inevitable — as the dataset grew, the AI slowed down, forgot details, and hallucinated more frequently.

## Growing Sophistication — How I Solved the Problems
The first shift was specificity: dedicating each chat session to a single task. This improved results but didn’t scale — every session still required priming with context.

I experimented with giving AI direct access to a Git repo or Google Drive folder. It worked inconsistently, sometimes pulling outdated data or forgetting how to use the integration entirely. Worse, cached older versions could go unnoticed.

The solution was to control the context myself. I built a script to collate all markdown into a single “canon” file and upload it into each session. While better, the plain text dump had issues:

Slow for the AI to model from.
Hard to parse and reference.
No sense of relative importance.

This evolved into a structured JSON dataset (“canon_compilation.txt”) containing:

Global metadata and instructions.
A list of every file, with paths, titles, tags, and internal links.
Full YAML frontmatter and content.

I also created two dedicated prompts:

The Loremaster — for world-scale consistency and thematic checks.
The Outliner — for manuscript structure and scene planning.

Supporting tools — “find-and-replace.py”, “linker.py”, and the “buildashandhollow” script — automated updates, link checks, and one-command builds.

## “canon_compilation.txt” — What It Is
Structured Canon Snapshot — The “canon_compilation.txt” is generated by running “buildashandhollow”, which calls “consolidate.py”.

It follows a strict JSON schema:

```json
{
  "directive": {
    "prompt": "string",
    "canon_version": "string",
    "compilation_generated_on": "string",
    "total_files_included": number
  },
  "canon_data": [
    {
      "path": "string",
      "title": "string",
      "links": ["string"],
      "tags": ["string"],
      "content": "string"
    }
  ]
}
```

Why it’s better:

Explicit context before any lore is processed.
Version control ensures exact snapshot alignment.
Traceability to the precise source file.
Relationship mapping through “links” and “tags”.
Clean separation of content and metadata.
Scalable for future metadata expansion.

This structure makes the canon navigable, queryable, and traceable, ensuring AI analysis is grounded in a reliable dataset.

## The Master Workflow — How I Use It
The Master Workflow ensures every AI session starts with an accurate, navigable snapshot of the world.

Steps:

Build snapshot — Run “buildashandhollow” to generate a fresh “canon_compilation.txt”.
Load and verify — Paste into ChatGPT, confirm version, and let the AI synthesise the world.
Define the session — Set a goal, choose focus (entire canon or 2–5 files), provide context.
Select mode — Loremaster for big-picture checks; Focused for targeted work.
Discussion — I present my idea, AI challenges it, and we refine until I’m happy with the synthesis.
Lock-in and impact analysis — I confirm the idea, and the AI runs an impact analysis, identifying which files must change.
Suggested changes — AI recommends updates to those files, which can include references, links, and descriptions.
File updates — I work on the file in Canvas, finalise the change, and paste the updated file into Obsidian.
Next iteration — AI moves on to the next file in the change list.

Impact: Tasks that once took hours of manual cross-referencing now take minutes. AI acts as a context-aware editorial partner — fast, precise, and under full creative control.

## Conclusion — The Transformation
Integrating AI into my creative process has been nothing short of transformative. It’s taken me from a painstaking, manual grind to a streamlined, structured, and collaborative cycle. But it’s not a plug-and-play solution — I’ve learned that AI must be treated as a powerful but fallible partner. Blindly following its output can lead to subtle errors and inconsistencies.

Through trial, error, and refinement, I’ve developed a workflow that maximises AI’s strengths — speed, pattern recognition, and tireless cross-referencing — while mitigating its weaknesses with human oversight and structured data. I remain in full creative control, but now I can move from idea to implementation in a fraction of the time. The result is a process that not only safeguards the integrity of Ash and Hollow’s vast canon but also accelerates my ability to expand and enrich it.
